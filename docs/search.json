[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Workshop for Bioscience",
    "section": "",
    "text": "Welcome\n\nWelcome to the R workshop for Bioscience!\nThe workshop is intended for any Bioscience researchers with limited prior experience in R who is interested in learning how to use R to do data wrangling, data visualization, common statistical analyses.\nWorkshop materials in the github repository Rworkshop\n\n\n\nLearning objectives\nAt the end of the class, students will be able to:\n\nnavigate R and R studio interface\nset up project and coding environment for data analysis\nefficiently handle datasets\nperform common data visualizations used in Bioscience\nconduct descriptive analysis in R\nconduct regression analysis in R (if time permits)\n\n\n\nSchedule\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 9:10\nIntroduction\n\n\n9:10 - 9:45\nSession 1: Get Started with R\n\n\n9:45 - 10:00\nQuestions + Coffee break\n\n\n10:00 - 10:45\nSession 2: Working with Data\n\n\n10:45 - 11:00\nQuestions + Coffee break\n\n\n11:00 - 11:45\nSession 3: Statistical Analysis in R\n\n\n11:45 - 12:00\nConclusions and Questions\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nInstall R and R Studio\n\nWindows operating system\n\ninstall R, https://cran.r-project.org/bin/windows/base/\ninstall RStudio, https://posit.co/download/rstudio-desktop/#download\n\nmacOS operating system\n\ninstall R, https://cran.r-project.org/bin/macosx/\ninstall RStudio, https://posit.co/download/rstudio-desktop/#download\n\n\nVerify access to the course page, https://kuan-liu.github.io/Rworkshop/\nClone or download the workshop repository: https://github.com/Kuan-Liu/Rworkshop\n\n\n\n\n\n\nReference and resource\n\nR for Data Science by Hadley Wickham & Garrett Grolemund\nggplot2: elegant graphics for data analysis by Hadley Wickham\n\n\n\n\n\n\n\n\nR for Data Science\nggplot2: elegant graphics for data analysis\n\n\n\n\n\n\n\n\n\n\n\nAbout me\n\nI am an Assistant Professor in Biostatistics and Health Services Research at the University of Toronto\nMy primary research focuses on developing methodology for statistical inference with complex longitudinal data in comparative effectiveness research\n\ncausal inference\nBayesian statistics\nlongitudinal data analysis\njoint modelling\nbias analysis\n\nMy ties to forestry and ecology üå≤"
  },
  {
    "objectID": "session1.html",
    "href": "session1.html",
    "title": "1. Getting started with R",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nlearn about R and RStudio work environment\nuse R as a calculator\nunderstand objects in R\nlearn about simple iterative programming in R"
  },
  {
    "objectID": "session1.html#r-and-rstudio",
    "href": "session1.html#r-and-rstudio",
    "title": "1. Getting started with R",
    "section": "1.1 R and RStudio",
    "text": "1.1 R and RStudio\n\nR is a language and environment for statistical computing and graphics (https://cran.r-project.org/manuals.html).\nMany users of R like to use RStudio as the preferred interface for programming in R.\nRStudio is an Integrated Development Environment (IDE) for R.\n\neasy to navigate\nlots of point and click features and customizations\nRstudio is not just for R\n\n\n\n\nRStudio layout\nWhen you open RStudio, your interface is made up of four panes as shown below. These can be organised via menu options View > Panes >\n\n\n\nRStudio layout\n\n\n\n\nR Packages\n\nPackages are the fundamental units of reproducible R code.\nThey include reusable R functions, the documentation that describes how to use them, and sample data.\nWe install the package using install.packages() function or we can use the Package tab in Rstudio.\nOnce we have the package installed, we must load the functions from this library so we can use them within R.\n\n\n# install.packages(tidyverse, dependencies = T)\n  library(tidyverse) # load package library\n\n\n\nR script\n\nWe can run code in the console at the prompt where R will evaluate it and print the results.\nbest practice write code in a new script file so it can be saved, edited, and reproduced.\nTo open a new script, we select File > New File > R Script.\nTo ‚Äúrun code‚Äù that was written in the script file, you can highlight the code lines you wish be evaluated and\n\npress CTRL-Enter (windows)\nCmd+Return (Mac).\n\nAdditionally, You can comment or uncomment script lines by pressing\n\nCtrl+Shift+C (windows)\nCmd+Shift+C (Mac).\n\nThe comment operator in R is #.\nYou can find more RStudio default keyboard shortcuts here.\n\n\n\nCustomization\n\nYou can customize your RStudio session under the Options dialog Tools > Global Options menu (or RStudio > Preferences on a Mac).\nA list of customization categories can be found here.\n\n\n\n\nWorking directory\n\nThe working directory is the default location where R will look for files you want to load and where it will put any files you save.\nYou can use function getwd() to display your current working directory\nand use function setwd() to set your working directory to a new folder on your computer.\n\n\n\ngetwd() #show my current working directory;\n\n[1] \"D:/GitHub/Rworkshop\"\n\n\n\n\nGetting help with R\n\nThe help section of R is extremely useful if you need more information about the packages and functions that you are currently loaded.\nYou can initiate R help using the help function help() or ?, the help operator.\n\n\nhelp(ggplot)"
  },
  {
    "objectID": "session1.html#basic-r",
    "href": "session1.html#basic-r",
    "title": "1. Getting started with R",
    "section": "1.2 Basic R",
    "text": "1.2 Basic R\n\nIn this subsection, I will briefly outline some common R functions and commands for arithmetic, creating and working with objects such as vector and matrix\n\n\n\n\n\n\n\n\nR is case sensitive.\nCommands are separated by a newline in the console.\nThe # character can be used to make comments. R does not execute the rest of the line after the # symbol - it ignores it.\nPrevious commands can be accessed via the up and down arrow keys on the keyboard.\nWhen naming in R, avoid using spaces and special characters (i.e., !@#$%^&*()_+=;:‚Äô‚Äú<>?/) and avoid leading names with numbers.\nit‚Äôs common to see error and warning messages pop up as output in Console\n\nbest solution: searching for online answers!\n\n\n\n\n\n\nArithmetic\n\n2+3\n3-2\n2*3\n2^3\n3/2\n2 + (2 + 3) * 2 - 5\n\n\npi\n\n[1] 3.141593\n\nexp(1)\n\n[1] 2.718282\n\nexp(3)\n\n[1] 20.08554\n\nlog(exp(1), base = exp(1)) #playing with Euler's number;\n\n[1] 1\n\nlog(3, base = exp(1)) #default natural logarithms;\n\n[1] 1.098612\n\nlog(3, base = 10)\n\n[1] 0.4771213\n\nlog10(3)\n\n[1] 0.4771213\n\nlog(-1) #warning message;\n\n[1] NaN\n\n\n\nSome of the other available useful functions are: abs(), sqrt(), ceiling(), floor(), trunc(), round() .\n\n\n\nWorking with objects\n\nR is an object-oriented programming language.\nWe can create objects and save them in our workspace & environment\n\n\n\n\n\n\n\nAn object is composed of three parts: 1) a value we‚Äôre interested in 2) an identifier and 3) the assignment operator.\n\nValue: can take any forms\n\na number, a string of characters, a data frame, a plot or a function\n\nidentifier is the name you assign to the value.\nassignment operator resembles an arrow <- and is used to link the value to the identifier.\n\n\n\n\n\n# Creating a scalar called \"a\" and assigning a value of 2\na<-2\n\n# Creating a scalar called \"b\" and assigning a value of 3\nb<-3\n\n# Adding \"a\" and \"b\" and saving under \"d\"\nd<-a+b\n\n# Printing the value of \"d\"\nd\n\n[1] 5\n\n# Updating the value of a scalar\n# Adds 5 to the old value of \"a\" and saves it again under the name \"a\".\na<-a+5 \na\n\n[1] 7\n\n\n\n\nLogic check\n\nTRUE or FALSE?\n\n\n\n\nOperator\n\n\n\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\nx | y\nx or y\n\n\nx & y\nx and y\n\n\n\n\na<5   # checks if x is less than 5 or not\na>5   # checks if x is greater than 5 or not\na<=5  # less or equal\na>=5  # greater or equal\na==4   #( == stands for equal)\na!=4   #( != stands for not equal)\n\n\n\nData structures\n\nVectors\nMatrices\narrays\nData frames\nList\n\n\n\n\ndata structures in R - R in Action, Chapter 2\n\n\n\nVectors\n\nvectors can contain same type or mixed type elements.\nvector.name <- c(value1, value2, value3, ...).\nThe function c() means combine or concatenate and is used to create vectors.\nTypes of elements:\n\nnumeric(double)\ninteger\ncharacter\nlogical: TRUE, FALSE\nSpecial values: NA(not available or missing), NULL(empty), NaN(not a number), Inf(infinite)\n\n\n\nYou can use typeof() or class() to examine an object‚Äôs type, or use an is() function.\n\n\n# a vector of a single numeric element;\nx <- 3\nx\n\n[1] 3\n\ntypeof(x) #also try class(x);\n\n[1] \"double\"\n\nis(x)\n\n[1] \"numeric\" \"vector\" \n\n# a character vector\nx <- c(\"red\", \"green\", \"yellow\")\nx\n\n[1] \"red\"    \"green\"  \"yellow\"\n\ntypeof(x) #also try class(x);\n\n[1] \"character\"\n\nlength(x)\n\n[1] 3\n\nnchar(x) #number of characters for each element;\n\n[1] 3 5 6\n\n# encode a vector as a factor (or category);\ny <- factor(c(\"red\", \"green\", \"yellow\", \"red\", \"red\", \"green\"))\ny\n\n[1] red    green  yellow red    red    green \nLevels: green red yellow\n\nattributes(y)\n\n$levels\n[1] \"green\"  \"red\"    \"yellow\"\n\n$class\n[1] \"factor\"\n\nas.numeric(y) # we can return factors with numeric labels;\n\n[1] 2 1 3 2 2 1\n\n# we can update the levels;\nlevels(y)<- c(\"green\",\"yellow\",\"red\")\nattributes(y)\n\n$levels\n[1] \"green\"  \"yellow\" \"red\"   \n\n$class\n[1] \"factor\"\n\n# we can also label numeric vector with factor levels;\nz <- factor(c(1,2,3,1,1,2), levels = c(1,2,3), labels = c(\"red\", \"green\",\"yellow\"))\nz\n\n[1] red    green  yellow red    red    green \nLevels: red green yellow\n\nattributes(z)\n\n$levels\n[1] \"red\"    \"green\"  \"yellow\"\n\n$class\n[1] \"factor\"\n\n# using the repeat command;\n# the following line repeats 3, 5 times\nrep(x=3,each=5)   \n\n[1] 3 3 3 3 3\n\n# using sequence command;\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from=1, to=10, by=1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrep(x=1:2, each = 2)\n\n[1] 1 1 2 2\n\n\n\nLogical check for a vector\n\nJust like a scalar, we can evaluate logical conditions using a vector as well.\nThis is an element-wise operation.\nR will check every element of the vector\nThe output will be a TRUE/FALSE vector.\n\n\n\n#Let's star with a new vector which has 5 elements\nx<- c(3,6,2,8,10)\nx>5   \n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nx==2\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\nsum(x>5)\n\n[1] 3\n\n\n\nselect or remove elements from a vector\n\nwe use the open bracket [ ] after the vector and use index to operate.\n\n\n\n#Starting with same x vector\n# x= c(3,6,2,8,10)\n\nx[1]          # gives us the first element\n\n[1] 3\n\nx[c(1,3,4) ]   # return the 1st, 3rd and 4th element\n\n[1] 3 2 8\n\nx[-1]         # remove the first element\n\n[1]  6  2  8 10\n\nx[-1:-2]      # remove first and second elements\n\n[1]  2  8 10\n\nx[-c(1,2)]\n\n[1]  2  8 10\n\n\n\nCalculating summary statistics of a vector\n\n\nset.seed(123)\nr <- sample(x = 1:100, size = 100, replace = TRUE)\nmean(r)  #calculate the mean of a vector\n\n[1] 52.15\n\nvar(r)   #variance of a vector\n\n[1] 874.7348\n\nsd(r)    #standard deviation of a vector\n\n[1] 29.57592\n\nmin(r)   #minimum of a vector\n\n[1] 4\n\nmax(r)   #maximum of a vector\n\n[1] 99\n\nmedian(r)#median\n\n[1] 50\n\nrange(r) #range\n\n[1]  4 99\n\n\n\n\nMatrices\n\nmatrices have two dimensions, rows and columns\n\n\n# matrix in R;\nmatrix(data = 1:16, nrow=4, ncol=4, byrow=TRUE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nmatrix(data = 1:16, nrow=4, ncol=4, byrow=FALSE)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n# creating matrix using diagonal; \ndiag(c(1,1,1))\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n# matrix calculation;\nX <- matrix(data = 1:16, nrow=4, ncol=4, byrow=TRUE)\ndiag(X)\n\n[1]  1  6 11 16\n\nt(X) #transpose;\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nY <- matrix(seq(1,32, by=2), nrow=4, byrow=T)\nY\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    9   11   13   15\n[3,]   17   19   21   23\n[4,]   25   27   29   31\n\n# matrix operation;\nY + X\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    5    8   11\n[2,]   14   17   20   23\n[3,]   26   29   32   35\n[4,]   38   41   44   47\n\nY - X\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1    2    3\n[2,]    4    5    6    7\n[3,]    8    9   10   11\n[4,]   12   13   14   15\n\n3 * X\n\n     [,1] [,2] [,3] [,4]\n[1,]    3    6    9   12\n[2,]   15   18   21   24\n[3,]   27   30   33   36\n[4,]   39   42   45   48\n\nX * Y\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   15   28\n[2,]   45   66   91  120\n[3,]  153  190  231  276\n[4,]  325  378  435  496\n\nX %*% Y #inner product;\n\n     [,1] [,2] [,3] [,4]\n[1,]  170  190  210  230\n[2,]  378  430  482  534\n[3,]  586  670  754  838\n[4,]  794  910 1026 1142\n\n\n\n\nData frames\n\nA data frame is a group of vectors of the same length.\nTwo dimensions: columns are variables and rows are observations\nUnlike matrix, a data frame can contain different data types (e.g., numeric or character)\n\n\nsite_id <- c(\"A\", \"B\", \"C\", \"D\")  #identifies the soil sampling site;\nsoil_pH <- c(6.1, 7.4, 5.1, 6)  #soil pH\nnum_species <- c(17, 23, 7, 15)  #number of species\ntreated <- c(\"yes\", \"yes\", \"no\", \"no\")  #treatment status;\n\n# use data.frame function to create a data frame;\nsoil_data <- data.frame(site_id, soil_pH, num_species, treated)\n\n# view data;\nsoil_data\n\n  site_id soil_pH num_species treated\n1       A     6.1          17     yes\n2       B     7.4          23     yes\n3       C     5.1           7      no\n4       D     6.0          15      no\n\nstr(soil_data)\n\n'data.frame':   4 obs. of  4 variables:\n $ site_id    : chr  \"A\" \"B\" \"C\" \"D\"\n $ soil_pH    : num  6.1 7.4 5.1 6\n $ num_species: num  17 23 7 15\n $ treated    : chr  \"yes\" \"yes\" \"no\" \"no\"\n\ndim(soil_data)\n\n[1] 4 4\n\nnrow(soil_data)\n\n[1] 4\n\nncol(soil_data)\n\n[1] 4\n\ncolnames(soil_data)\n\n[1] \"site_id\"     \"soil_pH\"     \"num_species\" \"treated\"    \n\n\n\n\nLists\n\nhighly flexible objects\nlists can contain anything as their elements\n\n\nexample_list <- list(\n  num = sep(from=1, to=10, by=2),\n  char = c(\"apple\", \"pineapple\"),\n  logic = c(TRUE, TRUE, FALSE)\n)"
  },
  {
    "objectID": "session1.html#advanced-topics---iterative-programming",
    "href": "session1.html#advanced-topics---iterative-programming",
    "title": "1. Getting started with R",
    "section": "1.3 Advanced topics - iterative programming",
    "text": "1.3 Advanced topics - iterative programming\n\nif statements in R\n\nIf statements in R has got this following structure if (condition){expression}\n\nA simple example:\n\nx<-3\nif(x==3){print(\"x is 3\")}\n\n[1] \"x is 3\"\n\n\n\n\nif else statement\n\nif(condition){\n  expression1\n  } else {\n  expression2\n}\n\n\nwe can also use ifelse() function, ifelse(condition, expression 1, expression 2)\n\n\ny <- c(6:-4)\nsqrt(y)  #- gives warning\n\nWarning in sqrt(y): NaNs produced\n\n\n [1] 2.449490 2.236068 2.000000 1.732051 1.414214 1.000000 0.000000      NaN\n [9]      NaN      NaN      NaN\n\nsqrt(ifelse(y >= 0, x, NA))  # no warning\n\n [1] 1.732051 1.732051 1.732051 1.732051 1.732051 1.732051 1.732051       NA\n [9]       NA       NA       NA\n\n\n\n\nmultiple conditions\n\nif (condition1) {\n    expression1\n} else if (condition2) {\n    expression2\n} else if (condition3) {\n    expression3\n} else {\n    expression4\n}\n\n\n# current value of x is 3\nif(x==4){\n  print(\"x is 4\")\n}else if (x>4){\n  print(\"x is greater than 4\")\n}else if (x<4){\n    print(\"x is less than 4\")\n  }\n\n[1] \"x is less than 4\"\n\n\n\n\nFor Loops\n\nperform a particular action for every iteration of some sequence\n\n\nfor (i in sequence){\n  expression\n}\n\n\na simple example\n\n\nfor (month in 1:12) {\n    print(paste('Month:', month))\n}\n\n[1] \"Month: 1\"\n[1] \"Month: 2\"\n[1] \"Month: 3\"\n[1] \"Month: 4\"\n[1] \"Month: 5\"\n[1] \"Month: 6\"\n[1] \"Month: 7\"\n[1] \"Month: 8\"\n[1] \"Month: 9\"\n[1] \"Month: 10\"\n[1] \"Month: 11\"\n[1] \"Month: 12\"\n\n\n\na slightly more complex example combining for loop and if statements - counting even numbers\n\n\nx <- c(2,5,3,9,8,11,6)\ncount <- 0\nfor (val in x) {\nif(val %% 2 == 0)  count = count+1\n}\nprint(count)\n\n[1] 3\n\n\n\n\napply family\n\napply family functions can be used in the same way as a for loop\napply()\n\napply over the margins of an array (e.g.¬†the - rows or columns of a matrix)\n\nlapply()\n\napply over an object and return list\n\nsapply()\n\napply over an object and return a simplified object (an array) if possible\n\nvapply()\n\nsimilar to sapply but you specify the type of object returned by the iterations\n\nmapply()\n\nmultivariate version of sapply()\n\ntapply()\n\nused to apply a function over subsets of a vector\n\n\n\n# a matrix with apply;\nmymatrix<-matrix(1:9,nrow=3)\nmymatrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# calculate row sum;\napply(X=mymatrix,MARGIN=1,FUN = sum) \n\n[1] 12 15 18\n\n# a list with lapply\nmylist<-list(A=matrix(1:9,nrow=3),B=1:5,C=8)\n\n# calculate sum for each element of the list;\nlapply(mylist,FUN = sum)\n\n$A\n[1] 45\n\n$B\n[1] 15\n\n$C\n[1] 8\n\n# calculate sum for each element of the list and simplify it to a vector;\nsapply(mylist, FUN = sum)\n\n A  B  C \n45 15  8 \n\n\n\n\n\n\n\n\nTips\n\n\n\n\nWhere possible, use vectorized operations instead of for loops to make code faster and more concise.\nUse functions such as apply instead of for loops to operate on the values in a data structure.\n\n\n\n\n\nEffectively use loops in statistically modelling\n\nThis can be handy in statistical modelling!\nData: Motor Trend Car Road Tests\n\nA data frame with 32 observations on 11 (numeric) variables.\n\nmpg Miles/(US) gallon\ncyl Number of cylinders\ndisp Displacement (cu.in.)\nhp Gross horsepower\ndrat Rear axle ratio\nwt Weight (1000 lbs)\nqsec 1/4 mile time\nvs Engine (0 = V-shaped, 1 = straight)\nam Transmission (0 = automatic, 1 = manual)\ngear Number of forward gears\ncarb Number of carburetors\n\n\n\n\nlibrary(DT)\ndatatable(mtcars,\n          options = list(dom = 't'))\n\n\n\n\n\n\n\n# creating a list a variables that are predictive of the fuel consumption;\n\npredictors <- colnames(mtcars)[-1]\npredictors\n\n [1] \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\" \"carb\"\n\n#run unadjusted regression analysis for each predictor;\n# m1 <- lm(mpg~cyl,data = mtcars)\n# m2 <- lm(mpg~disp,data = mtcars)\n# m3 <- lm(mpg~hp,data = mtcars)\n\n# make a list of model formulars: list(mpg ~ cyl, mpg ~ disp, ...);\nlist_model_formulas <- sapply(predictors,function(x)as.formula(paste('mpg~',x)))\n\n# making a list of unadjusted models;\nlist_models <- lapply(list_model_formulas,function(x){lm(x,data=mtcars)})\n\n#extract model results;\nresults <- lapply(list_models, function(x){return(summary(x)$coef)})\nresults\n\n$cyl\n            Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 37.88458  2.0738436 18.267808 8.369155e-18\ncyl         -2.87579  0.3224089 -8.919699 6.112687e-10\n\n$disp\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 29.59985476 1.229719515 24.070411 3.576586e-21\ndisp        -0.04121512 0.004711833 -8.747152 9.380327e-10\n\n$hp\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 30.09886054  1.6339210 18.421246 6.642736e-18\nhp          -0.06822828  0.0101193 -6.742389 1.787835e-07\n\n$drat\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -7.524618   5.476663 -1.373942 0.1796390847\ndrat         7.678233   1.506705  5.096042 0.0000177624\n\n$wt\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 37.285126   1.877627 19.857575 8.241799e-19\nwt          -5.344472   0.559101 -9.559044 1.293959e-10\n\n$qsec\n             Estimate Std. Error    t value   Pr(>|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\n\n$vs\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 16.616667   1.079711 15.389917 8.846603e-16\nvs           7.940476   1.632370  4.864385 3.415937e-05\n\n$am\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 17.147368   1.124603 15.247492 1.133983e-15\nam           7.244939   1.764422  4.106127 2.850207e-04\n\n$gear\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 5.623333   4.916379 1.143796 0.261753365\ngear        3.923333   1.308131 2.999191 0.005400948\n\n$carb\n             Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 25.872334  1.8368072 14.08549 9.218370e-15\ncarb        -2.055719  0.5685456 -3.61575 1.084446e-03"
  },
  {
    "objectID": "session2.html",
    "href": "session2.html",
    "title": "2. Working with Data",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nLoading, exploring and saving data\nLearn to manipulate data frames with tidyverse\nOverview of ggplot2 for data visualization"
  },
  {
    "objectID": "session2.html#data-import-and-export",
    "href": "session2.html#data-import-and-export",
    "title": "2. Working with Data",
    "section": "2.1 Data import and export",
    "text": "2.1 Data import and export\n\npackage readr reads txt, csv, Rdata (or rda).\npackage haven reads SPSS, Stata, and SAS files.\npackage readxl reads excel files (both .xls and .xlsx).\n\n\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE, fig.align=\"center\")\noptions(scipen = 999, pillar.print_max = Inf)\nco2 <- read_csv(file = \"co2_mm_gl_clean.csv\")\n\n\nLooking at the data\n\n\n\nco2\nLook at the whole data frame\n\n\n\n\nhead(co2)\nLook at the first few rows\n\n\ntail(co2)\nLook at the last few rows\n\n\ncolnames(co2)\nNames of the columns in the data frame\n\n\nattributes(co2)\nAttributes of the data frame\n\n\ndim(co2)\nDimensions of the data frame\n\n\nncol(co2)\nNumber of columns\n\n\nnrow(co2)\nNumber of rows\n\n\nsummary(co2)\nSummary statistics\n\n\nstr(co2)\nStructure of the data frame\n\n\n\n\nlibrary(DT)\ndatatable(co2)\n\n\n\n\n\n\n\nstr(co2)\n\nspc_tbl_ [526 √ó 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year   : num [1:526] 1979 1979 1979 1979 1979 ...\n $ month  : num [1:526] 1 2 3 4 5 6 7 8 9 10 ...\n $ decimal: num [1:526] 1979 1979 1979 1979 1979 ...\n $ average: num [1:526] 337 337 338 338 338 ...\n $ stddev : num [1:526] 0.1 0.09 0.1 0.11 0.04 0.17 0.27 0.29 0.2 0.22 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   month = col_double(),\n  ..   decimal = col_double(),\n  ..   average = col_double(),\n  ..   stddev = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nsummary(co2) \n\n      year          month           decimal        average     \n Min.   :1979   Min.   : 1.000   Min.   :1979   Min.   :334.4  \n 1st Qu.:1989   1st Qu.: 3.250   1st Qu.:1990   1st Qu.:353.2  \n Median :2000   Median : 6.000   Median :2001   Median :369.7  \n Mean   :2000   Mean   : 6.481   Mean   :2001   Mean   :372.6  \n 3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:2012   3rd Qu.:391.3  \n Max.   :2022   Max.   :12.000   Max.   :2023   Max.   :418.5  \n     stddev       \n Min.   :0.03000  \n 1st Qu.:0.08000  \n Median :0.10000  \n Mean   :0.09894  \n 3rd Qu.:0.11000  \n Max.   :0.29000  \n\n\n\n\nRenaming variables\n\ncolnames(co2)[5]\n\n[1] \"stddev\"\n\ncolnames(co2)[5] <- \"sd\"\ncolnames(co2)\n\n[1] \"year\"    \"month\"   \"decimal\" \"average\" \"sd\"     \n\n\n\n\nCreating new variables\n\nco2$lowerbound_1sd <- co2$average - 1*co2$sd\nco2$upperbound_1sd <- co2$average + 1*co2$sd\nhead(co2)\n\n# A tibble: 6 √ó 7\n   year month decimal average    sd lowerbound_1sd upperbound_1sd\n  <dbl> <dbl>   <dbl>   <dbl> <dbl>          <dbl>          <dbl>\n1  1979     1   1979.    337.  0.1            336.           337.\n2  1979     2   1979.    337.  0.09           337.           337.\n3  1979     3   1979.    338.  0.1            338.           338.\n4  1979     4   1979.    338.  0.11           338.           338.\n5  1979     5   1979.    338.  0.04           338.           338.\n6  1979     6   1979.    337.  0.17           337.           338.\n\n\n\n\nSubsetting data\n\n# extracting data between 2020 and 2022;\nco2_2020_2022 <- subset(co2, year >= 2020 & year <=2022)\n\n# subletting directly from dataframe using index;\n# try at home;\n# co2_2020_2022 <- CO2[CO2year >= 2020 & year <=2022, ]  # Select observations with year between 2020 and 2022;\n\n\nwe can export data from r using write function\n\n\nwrite.csv(co2, file = 'co2_mm_gl_clean2.csv')"
  },
  {
    "objectID": "session2.html#data-manipulation-with-tidyverse-a-crush-introduction",
    "href": "session2.html#data-manipulation-with-tidyverse-a-crush-introduction",
    "title": "2. Working with Data",
    "section": "2.2 Data manipulation with tidyverse (a crush introduction)",
    "text": "2.2 Data manipulation with tidyverse (a crush introduction)\n\n\n\n\n\n\nWhat is the Tidyverse?\nThe tidyverse consists of a few key packages: - dplyr: data manipulation - ggplot2: data visualization - tibble: tibbles, a modern re-imagining of data frames - tidyr: data tidying - readr: data import - purrr: functional programming, e.g.¬†alternate approaches to apply\n\n\n\n\nPipe operator %>%\nPipes are operators that send what comes before the pipe to what comes after.\nfrequently used in tidyverse!\n\n\nSelecting Columns\n\nExample: using %>% to subset data\n\nselect() in dplyr subset columns by names\nfilter() subset rows using column values\n\n\n\n# selecting columns;\nco2 %>%\n  select(year, month, average) %>%\n  head()\n\n# drop some variables;\nco2 %>%\n  select(-month) %>%\n  head()\n\nSometimes, we have a lot of variables to select, and if they have a common naming scheme, this can be very easy.\n\nco2 %>%\n  select(contains(\"bound\")) %>%\n  head()\n\n\nOver helpful functions to be used within select()\n\nstarts_with: starts with a prefix\nends_with: ends with a suffix\ncontains: contains a literal string\nmatches: matches a regular expression\nnum_range: a numerical range like wk1, wk2, wk3.\n\nselect(num_range(\"wk\", 1:3))\n\neverything: all variables.\n\n\n\n\nselecting rows\n\n# selecting observations in year 2022;\nco2 %>%\n  select(year, month, average) %>%\n  filter(year == 2022)\n\n# A tibble: 10 √ó 3\n    year month average\n   <dbl> <dbl>   <dbl>\n 1  2022     1    417.\n 2  2022     2    418.\n 3  2022     3    418.\n 4  2022     4    418.\n 5  2022     5    418.\n 6  2022     6    417.\n 7  2022     7    416.\n 8  2022     8    414.\n 9  2022     9    415.\n10  2022    10    416.\n\n\n\n# selecting observations between 2020 and 2022;\nco2 %>%\n  select(year, month, average) %>%\n  filter(year <= 2022 & year >= 2020)\n\n\n\ncreating new variables\n\nExample: using %>% and mutate() to create new variable\n\n\nco2 %>%\n  mutate(lowerbound_2sd = average - 2*sd,\n         upperbound_2sd = average + 2*sd) %>%\n  head()\n\n# A tibble: 6 √ó 9\n   year month decimal average    sd lowerbound_1sd upperbound_‚Ä¶¬π lower‚Ä¶¬≤ upper‚Ä¶¬≥\n  <dbl> <dbl>   <dbl>   <dbl> <dbl>          <dbl>         <dbl>   <dbl>   <dbl>\n1  1979     1   1979.    337.  0.1            336.          337.    336.    337.\n2  1979     2   1979.    337.  0.09           337.          337.    337.    337.\n3  1979     3   1979.    338.  0.1            338.          338.    338.    338.\n4  1979     4   1979.    338.  0.11           338.          338.    338.    339.\n5  1979     5   1979.    338.  0.04           338.          338.    338.    338.\n6  1979     6   1979.    337.  0.17           337.          338.    337.    338.\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãupperbound_1sd, ¬≤‚Äãlowerbound_2sd,\n#   ¬≥‚Äãupperbound_2sd\n\n# creating new variables based on conditions of another variable;\n# suppose we want to create a year group variable;\n\nco2 %>% \n  mutate(year_group = case_when(\n    year < 1980 ~ '1970-1979',\n    1980 <= year & year < 1990 ~ '1980-1989',\n    1990 <= year & year < 2000 ~ '1990-1999',\n    2000 <= year & year < 2010 ~ '2000-2009',\n    2010 <= year & year < 2020 ~ '2010-2019',\n    2020 <= year & year < 2030 ~ '2020-2029',\n  )) %>%\n  head()\n\n# A tibble: 6 √ó 8\n   year month decimal average    sd lowerbound_1sd upperbound_1sd year_group\n  <dbl> <dbl>   <dbl>   <dbl> <dbl>          <dbl>          <dbl> <chr>     \n1  1979     1   1979.    337.  0.1            336.           337. 1970-1979 \n2  1979     2   1979.    337.  0.09           337.           337. 1970-1979 \n3  1979     3   1979.    338.  0.1            338.           338. 1970-1979 \n4  1979     4   1979.    338.  0.11           338.           338. 1970-1979 \n5  1979     5   1979.    338.  0.04           338.           338. 1970-1979 \n6  1979     6   1979.    337.  0.17           337.           338. 1970-1979 \n\nco2 <- co2 %>% #updating the data object\n  mutate(year_group = case_when(\n    year < 1980 ~ '1970-1979',\n    1980 <= year & year < 1990 ~ '1980-1989',\n    1990 <= year & year < 2000 ~ '1990-1999',\n    2000 <= year & year < 2010 ~ '2000-2009',\n    2010 <= year & year < 2020 ~ '2010-2019',\n    2020 <= year & year < 2030 ~ '2020-2029',\n  ))\n\n\n\nGrouping and Summarizing Data\n\nwe can use group_by() and summarize() to help calculating group-based statistics\nExample: suppose we want to calculate average, min, and max co2 by years (aggregated over month)\n\n\nco2 %>%\n  select(year, average) %>%\n  group_by(year) %>%\n  summarise(\n    `mean co2 by month` = mean(average),\n    `min co2 by month` = min(average),\n    `max co2 by month` = max(average)\n  )\n\n# A tibble: 44 √ó 4\n    year `mean co2 by month` `min co2 by month` `max co2 by month`\n   <dbl>               <dbl>              <dbl>              <dbl>\n 1  1979                337.               334.               338.\n 2  1980                339.               337.               340.\n 3  1981                340.               338.               342.\n 4  1982                341.               338.               343.\n 5  1983                343.               341.               344.\n 6  1984                344.               342.               345.\n 7  1985                346.               343.               347.\n 8  1986                347.               345.               348.\n 9  1987                349.               347.               350.\n10  1988                351.               349.               352.\n11  1989                353.               350.               354.\n12  1990                354.               352.               356.\n13  1991                355.               353.               357.\n14  1992                356.               354.               358.\n15  1993                357.               354.               358.\n16  1994                358.               356.               360.\n17  1995                360.               358.               362.\n18  1996                362.               360.               363.\n19  1997                363.               360.               365.\n20  1998                366.               364.               367.\n21  1999                368.               365.               369.\n22  2000                369.               367.               370.\n23  2001                371.               368.               372.\n24  2002                373.               370.               374.\n25  2003                375.               373.               377.\n26  2004                377.               374.               378.\n27  2005                379.               377.               381.\n28  2006                381.               378.               383.\n29  2007                383.               380.               384.\n30  2008                385.               383.               387.\n31  2009                386.               384.               388.\n32  2010                389.               386.               390.\n33  2011                391.               388.               392.\n34  2012                393.               390.               394.\n35  2013                395.               393.               397.\n36  2014                397.               395.               399.\n37  2015                400.               397.               402.\n38  2016                403.               401.               405.\n39  2017                405.               403.               407.\n40  2018                408.               405.               409.\n41  2019                410.               408.               412.\n42  2020                412.               410.               414.\n43  2021                415.               412.               417.\n44  2022                417.               414.               418.\n\n\n\nwe observe an increasing trend of global co2 concenration over years.\n\n\n\nRenaming columns\n\n#example syntax;\ndata %>%\n  rename(new_name = oldname,\n         new_name2 = oldname2)\n\n\n\nReshaping datasets - wide vs long data\n - pivot function has three argumnets:\nThe function requires the following arguments - a data frame (e.g.¬†‚Äúwide‚Äù) - cols: name of the columns we wish to gather - names_to: name of the new column - values_to: name of the new column containing variable values\n\nsuppose we want to reshape the long co2 data to wide data with month 1 to 12 as columns\n\n\nco2_wide <- co2 %>%\n  select(year, month, average) %>%\n  pivot_wider(names_from = month,\n              names_prefix = \"mth\",\n              values_from = average)\n\nhead(co2_wide)\n\n# A tibble: 6 √ó 13\n   year  mth1  mth2  mth3  mth4  mth5  mth6  mth7  mth8  mth9 mth10 mth11 mth12\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  1979  337.  337.  338.  338.  338.  337.  336.  334.  335.  336.  337.  338.\n2  1980  339.  339.  340.  340.  340.  340.  338.  337.  337.  338.  339.  340.\n3  1981  340.  341.  341.  342.  341.  341.  339.  338.  338.  339.  340.  341.\n4  1982  341.  342.  342.  343.  342.  341.  340.  338.  338.  340.  341.  342.\n5  1983  342.  343.  343.  344.  344.  344.  342.  341.  341.  342.  343.  343.\n6  1984  344.  345.  345.  345.  345.  345.  343.  342.  342.  343.  344.  345 \n\n\n\n\nMerging data frames\n\nwe can also join two or multiple data frames together.\n\nleft_join()\n\nkeeps all the entries that are present in the left (first) table and excludes any that are only in the right table\n\nright_join()\n\nkeeps all the entries that are present in the right table and excludes any that are only in the left table.\n\ninner_join()\n\nkeeps only the entries that are present in both tables. inner_join is the only function that guarantees you won‚Äôt generate any missing entries.\n\nfull_join()\n\nkeeps all of the entries in both tables, regardless of whether or not they appear in the other table.\n\n\n\n\n\n\nR cheetsheet on merging data"
  },
  {
    "objectID": "session2.html#data-visualization-with-ggplot2",
    "href": "session2.html#data-visualization-with-ggplot2",
    "title": "2. Working with Data",
    "section": "2.3 Data visualization with ggplot2",
    "text": "2.3 Data visualization with ggplot2\n > Data visualization is key to data story telling.\n\nggplot2 is a powerful package that enable publication ready plots\n\neasy customization (over SAS)\nclear syntax and lots of online template\nlots of extensions on style\n\n\n\n\n\nHow ggplot2 works, https://codeahoy.com/learn/rtutorial/ch8/\n\n\n\n\n\n\n\n\nggplot components (from Wickham, 2009)\n\n\n\n\nlayer is a collection of geometric elements and statistical transformations.\naesthetic(aes) is ‚Äúsomething you can see‚Äù.\n\nx, y: variable along the x and y axis\ncolour: color of geoms according to data\nfill: the inside color of the geom\ngroup: what group a geom belongs to\nshape: the figure used to plot a point\nlinetype: the type of line used (solid, dashed, etc)\nsize: size scaling for an extra dimension\nalpha: the transparency of the geom\n\ngeometric elements (geoms), represent what you actually see in the plot: points, lines, polygons, etc.\n\ngeom_area() draws an area plot\ngeom_bar(stat=\"identity\") draws a bar chart\ngeom_line() draws a line\ngeom_point() draws a scatterplot\ngeom_rect(), draws rectangles\n\nscales map values in the data space to values in the aesthetic space.\n\nThis includes the use of colour, shape or size.\nScales also draw the legend and axes\n\ncoordinate(coord), describes how data coordinates\n\nprovides axes and gridlines to help read the graph.\n\nfacet specifies how to break up and display subsets of data as small multiples.\ntheme controls display style, like the font size and background colour.\n\n\n\n\nLayers\n\nggplot(aes(x = decimal, y = average), data = co2)\n\n\n\n\n\n\n\n\n\n\nGeometry\n\nggplot(aes(x = decimal, y = average), data = co2)+\n  geom_point() +\n  geom_line(color = \"blue\")\n\n\n\n\n\n\n\n\n\ncomparing to the plot provided on https://gml.noaa.gov/ccgg/trends/\n\n\n\n\n\n\n\n\nExpand To Learn About various geoms\n\n\n\n\n\n\ngeom_abline: Reference lines: horizontal, vertical, and diagonal\ngeom_area: Ribbons and area plots\ngeom_bar: Bar charts\ngeom_boxplot: A box and whiskers plot\ngeom_contour: 2d contours of a 3d surface\ngeom_count: Count overlapping points\ngeom_crossbar: Vertical intervals: lines, crossbars & errorbars\ngeom_curve: Line segments and curves\ngeom_density: Smoothed density estimates\ngeom_dotplot: Dot plot\ngeom_errorbar: Vertical intervals: lines, crossbars & errorbars\ngeom_errorbarh: Horizontal error bars\ngeom_freqpoly: Histograms and frequency polygons\ngeom_hex: Hexagonal heatmap of 2d bin counts\ngeom_histogram: Histograms and frequency polygons\ngeom_hline: Reference lines: horizontal, vertical, and diagonal\ngeom_jitter: Jittered points\ngeom_label: Text\ngeom_line: Connect observations\ngeom_linerange: Vertical intervals: lines, crossbars & errorbars\ngeom_map: Polygons from a reference map\ngeom_path: Connect observations\ngeom_pointrange: Vertical intervals: lines, crossbars & errorbars\ngeom_polygon: Polygons\ngeom_qq: A quantile-quantile plot\ngeom_qq_line: A quantile-quantile plot\ngeom_quantile: Quantile regression\ngeom_raster: Rectangles\ngeom_ribbon: Ribbons and area plots\ngeom_rug: Rug plots in the margins\ngeom_segment: Line segments and curves\ngeom_smooth: Smoothed conditional means\ngeom_step: Connect observations\ngeom_text: Text\ngeom_tile: Rectangles\ngeom_violin: Violin plot\ngeom_vline: Reference lines: horizontal, vertical, and diagonal\n\n\n\n\n\n\nlabs, axis, facet, and theme\n\nggplot(aes(x = month, y = average), data = co2)+\n  geom_point(alpha = 0.5) +  \n  geom_line(color = \"blue\") +\n  labs(\n    x = 'Month',\n    y = 'CO2 mole fraction (ppm)',\n    title = 'Global Monthly Mean CO2'\n  ) +\n  scale_x_continuous(breaks = c(1,5,9,12)) +\n  scale_y_continuous(breaks = seq(from=300,to=450,by=50)) +\n  facet_wrap(vars(year)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nadding statistics\n\nco2 %>%\n  filter(year == 2021) %>%\n  ggplot(aes(x = month, y = average))+\n  geom_point() +\n  geom_line(color=\"blue\") +\n  geom_smooth(formula = y ~ x, method = 'lm', color = \"red\") +\n  geom_smooth(formula = y ~ splines::bs(x,3), method = 'lm', color = \"orange\") +\n  scale_x_continuous(breaks = seq(1,12,1)) +\n  labs(x = 'Month', y = 'CO2 mole fraction (ppm)', title = 'Global Monthly Mean CO2 in 2021')+ \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolours in R\n\n\n\n\nrequire(RColorBrewer)\n\nLoading required package: RColorBrewer\n\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\n\n\n\n\nOther representative plots\n\nBoxplot\n\nSuppose we want to look at CO2 distribution by year\n\nggplot(data = co2, aes(x=year_group, y=average, fill=year_group)) + \n  geom_boxplot(alpha=0.3) +\n  scale_fill_brewer(palette=\"Reds\")+ \n  labs(x = 'Year Group', \n       y = 'CO2 mole fraction (ppm)', \n       title = 'Global Monthly Mean CO2 by Year Group',\n       fill = 'Year group')+ \n  theme_bw()\n\n\n\n\n\n\n\n\n\nHeatmaps\nwe are interested to look at the pattern of CO2 concentration by year and month\nthree data dimensions: year, month, and value of co2\n\n\nggplot(data = co2, aes(y=factor(year), x=factor(month), fill=average)) + \n  geom_tile(colour = \"white\") +\n  scale_x_discrete(position = \"top\") +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +  \n  labs(x = 'Month', \n       y = 'Year', \n       title = 'Global Monthly Mean CO2')+ \n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe R Graph Gallery, https://r-graph-gallery.com/index.html\n\n\n\n\nBest reference for plotting with ggplot2 in R, https://r-graph-gallery.com/index.html"
  },
  {
    "objectID": "session3.html",
    "href": "session3.html",
    "title": "3. Statistical analysis in R",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nlearn how to conduct descriptive analysis in R\nlearn how to run regression analysis in r"
  },
  {
    "objectID": "session3.html#descriptive-analysis",
    "href": "session3.html#descriptive-analysis",
    "title": "3. Statistical analysis in R",
    "section": "3.1 Descriptive Analysis",
    "text": "3.1 Descriptive Analysis\n\nDescriptive statistics\n\nmeasures of central tendency, variability, and distribution shape for continuous variables\nfrequency counts for categorical variables\n\n\n\n# basic summary using summary function;\nsummary(bird)\n\n    Family             MaxAbund          AvgAbund           Mass        \n Length:54          Min.   :  0.200   Min.   : 0.200   Min.   :   5.30  \n Class :character   1st Qu.:  5.402   1st Qu.: 1.340   1st Qu.:  20.68  \n Mode  :character   Median : 24.147   Median : 3.114   Median :  59.18  \n                    Mean   : 44.906   Mean   : 5.686   Mean   : 468.48  \n                    3rd Qu.: 43.581   3rd Qu.: 6.258   3rd Qu.: 461.73  \n                    Max.   :413.600   Max.   :47.598   Max.   :5296.23  \n     Diet             Passerine        Aquatic      \n Length:54          Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :0.000   Median :0.0000  \n                    Mean   :0.463   Mean   :0.2778  \n                    3rd Qu.:1.000   3rd Qu.:1.0000  \n                    Max.   :1.000   Max.   :1.0000  \n\n# updating binary and categorical variables to factors in R;\nbird <- bird %>%\n  mutate(across(c(Family, Diet, Passerine, Aquatic), factor))\n\nsummary(bird)\n\n                             Family      MaxAbund          AvgAbund     \n Owls                           : 2   Min.   :  0.200   Min.   : 0.200  \n Anhingas                       : 1   1st Qu.:  5.402   1st Qu.: 1.340  \n Auks& Puffins                  : 1   Median : 24.147   Median : 3.114  \n Babblers                       : 1   Mean   : 44.906   Mean   : 5.686  \n Blackbirds & Orioles           : 1   3rd Qu.: 43.581   3rd Qu.: 6.258  \n Cardinals& Buntings& Grosbreaks: 1   Max.   :413.600   Max.   :47.598  \n (Other)                        :47                                     \n      Mass                  Diet    Passerine Aquatic\n Min.   :   5.30   Insect     :20   0:29      0:39   \n 1st Qu.:  20.68   InsectVert : 2   1:25      1:15   \n Median :  59.18   Plant      : 2                    \n Mean   : 468.48   PlantInsect:18                    \n 3rd Qu.: 461.73   Vertebrate :12                    \n Max.   :5296.23                                     \n                                                     \n\n\n\n# writing our own summary functions;\nmysummary <- function(x, na.omit=FALSE){                \n  x <- x[!is.na(x)]                \n  m <- mean(x)  \n  median <- median(x)\n  n <- length(x)                \n  s <- sd(x)                \n  skew <- sum((x-m)^3/s^3)/n                \n  kurt <- sum((x-m)^4/s^4)/n - 3                \n  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))}\n\nsapply(bird[,c(\"MaxAbund\", \"AvgAbund\", \"Mass\")],mysummary)\n\n          MaxAbund  AvgAbund       Mass\nn        54.000000 54.000000  54.000000\nmean     44.905769  5.686179 468.475673\nstdev    73.468869  8.249669 945.410448\nskew      3.167159  3.243436   3.146062\nkurtosis 11.124409 11.820367  11.396989\n\n\n\nWe can use the describe.by() function from the psych package to generate summary statistics by group\n\n\nlibrary(psych)\n\nWarning: package 'psych' was built under R version 4.2.2\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\ndescribe.by(bird[,c(\"MaxAbund\", \"AvgAbund\", \"Mass\")], bird$Diet)\n\nWarning: describe.by is deprecated. Please use the describeBy function\n\n\n\n Descriptive statistics by group \ngroup: Insect\n         vars  n   mean     sd median trimmed   mad min     max   range skew\nMaxAbund    1 20  33.34  40.38  17.94   25.38 21.49 0.2  157.88  157.68 1.61\nAvgAbund    2 20   3.80   3.35   3.16    3.30  2.71 0.2   12.09   11.89 1.13\nMass        3 20 251.97 583.01  23.87   95.50 26.83 5.3 2400.00 2394.70 2.66\n         kurtosis     se\nMaxAbund     2.10   9.03\nAvgAbund     0.18   0.75\nMass         6.51 130.36\n------------------------------------------------------------ \ngroup: InsectVert\n         vars n   mean     sd median trimmed    mad   min    max  range skew\nMaxAbund    1 2   3.42   1.57   3.42    3.42   1.64  2.32   4.53   2.22    0\nAvgAbund    2 2   1.83   1.60   1.83    1.83   1.68  0.70   2.96   2.26    0\nMass        3 2 185.68 183.59 185.68  185.68 192.47 55.86 315.50 259.64    0\n         kurtosis     se\nMaxAbund    -2.75   1.11\nAvgAbund    -2.75   1.13\nMass        -2.75 129.82\n------------------------------------------------------------ \ngroup: Plant\n         vars n  mean    sd median trimmed   mad   min    max  range skew\nMaxAbund    1 2 28.15 18.78  28.15   28.15 19.68 14.88  41.43  26.55    0\nAvgAbund    2 2  3.99  1.33   3.99    3.99  1.39  3.04   4.93   1.88    0\nMass        3 2 81.19 83.98  81.19   81.19 88.04 21.81 140.58 118.77    0\n         kurtosis    se\nMaxAbund    -2.75 13.28\nAvgAbund    -2.75  0.94\nMass        -2.75 59.38\n------------------------------------------------------------ \ngroup: PlantInsect\n         vars  n   mean     sd median trimmed   mad   min     max   range skew\nMaxAbund    1 18  69.81 102.60  30.77   52.29 18.46  6.34  413.60  407.26 2.29\nAvgAbund    2 18   8.36  10.95   5.01    6.35  3.15  1.34   47.60   46.26 2.63\nMass        3 18 143.39 314.43  32.55   89.04 27.79 12.18 1144.29 1132.11 2.36\n         kurtosis    se\nMaxAbund     4.48 24.18\nAvgAbund     6.38  2.58\nMass         4.07 74.11\n------------------------------------------------------------ \ngroup: Vertebrate\n         vars  n    mean      sd median trimmed    mad    min     max   range\nMaxAbund    1 12   36.53   72.04   4.58   18.26   6.07   0.20  255.65  255.45\nAvgAbund    2 12    5.74   10.08   0.93    3.69   0.99   0.20   31.80   31.60\nMass        3 12 1428.61 1502.42 983.01 1172.78 780.58 119.37 5296.23 5176.85\n         skew kurtosis     se\nMaxAbund 2.29     4.16  20.79\nAvgAbund 1.63     1.17   2.91\nMass     1.39     0.95 433.71\n\n\n\nFrequency counts for categorical variables\n\n\ntable(bird$Diet)\n\n\n     Insect  InsectVert       Plant PlantInsect  Vertebrate \n         20           2           2          18          12 \n\ntable(bird$Diet, bird$Aquatic) #diet by whether or not the bird lives on or around water;\n\n             \n               0  1\n  Insect      14  6\n  InsectVert   1  1\n  Plant        2  0\n  PlantInsect 17  1\n  Vertebrate   5  7\n\nxtabs(~Diet + Aquatic, data = bird)\n\n             Aquatic\nDiet           0  1\n  Insect      14  6\n  InsectVert   1  1\n  Plant        2  0\n  PlantInsect 17  1\n  Vertebrate   5  7\n\n\n\n(Visual) Bar display of Abundance distribution\n\n\nggplot(bird) +\n    geom_bar(aes(x=Diet, y=AvgAbund, fill = Aquatic), \n             stat=\"identity\",\n             position = position_dodge()) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nMeasures of independence between\ntesting independence of the categorical variables\n\nchi-square test\nFisher exact\nCochran-Mantel‚ÄìHaenszel (taking into account of confounding from a third variable)\n\n\n\n# testing independence between Diet and Aquatic;\nmytable <- xtabs(~Diet + Aquatic, data = bird)\nchisq.test(mytable)\n\nWarning in chisq.test(mytable): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  mytable\nX-squared = 11.326, df = 4, p-value = 0.02313\n\nfisher.test(mytable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  mytable\np-value = 0.01129\nalternative hypothesis: two.sided\n\nmytable <- xtabs(~Diet + Aquatic + Passerine, data=bird)\nmantelhaen.test(mytable)\n\n\n    Cochran-Mantel-Haenszel test\n\ndata:  mytable\nCochran-Mantel-Haenszel M^2 = 3.2768, df = 4, p-value = 0.5126\n\n\n\nMeasures of correlations\n\nvariety of correlation coefficients, including Pearson, Spearman, Kendall\n\nPearson product moment correlation assesses the degree of linear relationship between two quantitative variables\nSpearman‚Äôs Rank Order correlation coefficient assesses the degree of relationship between two rank-ordered variables - nonparametric\nKendall‚Äôs Tau is also a nonparametric measure of rank correlation\n\nwe can also view correlation descriptively using correlation plots\n\n\n\ncont_var <- bird[,c(\"MaxAbund\", \"AvgAbund\", \"Mass\")]\ncor(cont_var, method = \"pearson\")\n\n          MaxAbund  AvgAbund      Mass\nMaxAbund 1.0000000 0.8828926 0.1851167\nAvgAbund 0.8828926 1.0000000 0.1661142\nMass     0.1851167 0.1661142 1.0000000\n\ncor(cont_var, method = \"spearman\")\n\n           MaxAbund   AvgAbund       Mass\nMaxAbund  1.0000000  0.9447282 -0.3262117\nAvgAbund  0.9447282  1.0000000 -0.3333778\nMass     -0.3262117 -0.3333778  1.0000000\n\ncor(cont_var, method = \"kendall\")\n\n           MaxAbund   AvgAbund       Mass\nMaxAbund  1.0000000  0.8097902 -0.2320867\nAvgAbund  0.8097902  1.0000000 -0.2376791\nMass     -0.2320867 -0.2376791  1.0000000\n\n# testing correlation significance;\ncor.test(bird$MaxAbund, bird$Mass, method = \"pearson\", alternative = \"two.side\")\n\n\n    Pearson's product-moment correlation\n\ndata:  bird$MaxAbund and bird$Mass\nt = 1.3584, df = 52, p-value = 0.1802\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.08695387  0.43148965\nsample estimates:\n      cor \n0.1851167 \n\ncor.test(bird$MaxAbund, bird$Mass, method = \"spearman\", alternative = \"two.side\")\n\n\n    Spearman's rank correlation rho\n\ndata:  bird$MaxAbund and bird$Mass\nS = 34793, p-value = 0.01607\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3262117 \n\ncor.test(bird$MaxAbund, bird$Mass, method = \"kendall\", alternative = \"two.side\")\n\n\n    Kendall's rank correlation tau\n\ndata:  bird$MaxAbund and bird$Mass\nz = -2.4769, p-value = 0.01325\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.2320867 \n\nlibrary(corrplot)\nM <- cor(cont_var, method = \"pearson\")\ncorrplot(M, \n         method=\"circle\",\n         type = c(\"upper\"),\n         tl.srt=45) #Text label color and rotation; \n\n\n\n\n\n\n\n\n\nT-test, comparison of groups\nA two-group independent t-test can be used to test the hypothesis that the two population means are equal\n\n\n t.test(MaxAbund ~ Aquatic, data=bird)\n\n\n    Welch Two Sample t-test\n\ndata:  MaxAbund by Aquatic\nt = 0.46858, df = 30.457, p-value = 0.6427\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -32.55446  51.95678\nsample estimates:\nmean in group 0 mean in group 1 \n       47.60054        37.89937 \n\n\n\nNonparametric tests of group difference\n\nIf you‚Äôre unable to meet the parametric assumptions of a t-test or ANOVA, you can turn to nonparametric approaches,\n\nThomogeneity of variance (i.e., the variability of the data in each group is similar).\nThe distribution is approximately normal.\n\ne.g.¬†Wilcoxon rank sum test (more popularly known as the Mann‚ÄìWhitney U test)\nwhether the observations are sampled from the same probability distribution (that is, whether the probability of obtaining higher scores is greater in one population than the other)\n\n\n\nwilcox.test(MaxAbund ~ Aquatic, data=bird) \n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  MaxAbund by Aquatic\nW = 324.5, p-value = 0.543\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nComparing more than two groups\n\nANOVA\nKruskal-Wallis test, nonparametric\n\n\n\nres.aov <- aov(MaxAbund ~ Diet, data=bird)\nsummary(res.aov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nDiet         4  18688    4672   0.856  0.497\nResiduals   49 267389    5457               \n\nkruskal.test(MaxAbund ~ Diet, data=bird)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  MaxAbund by Diet\nKruskal-Wallis chi-squared = 8.6127, df = 4, p-value = 0.07154"
  },
  {
    "objectID": "session3.html#linear-regression",
    "href": "session3.html#linear-regression",
    "title": "3. Statistical analysis in R",
    "section": "3.2 Linear regression",
    "text": "3.2 Linear regression\nWhat is regression?\n\nusing one or more predictor variables to model the distribution of one or more outcome variables\nFor a continuous outcome, we typically fit a linear regression model.\n\nOf course the relationship between outcome and predictors can be non-linear, in this case, we would consider fitting polynomial regression models or splines.\n\nFor a categorical outcome, we will fit a generalized linear regression. We will cover this topic in future sessions.\nFor a repeatedly measured outcome, we can fit a linear mixed-effect model (continuous outcome) or a generalized linear mixed-effect model (categorical outcome).\n\n\nNormal Models and Linear Regression\n\n\n\n\n\n\nConditional normal model\n\n\n\n\nGiven its mean and variance, an observation has a normal distribution\n\n\\[ Y_i \\mid \\mu_i, \\sigma^2_i \\sim N( \\mu_i, \\sigma^2_i) \\]\n\nThis is equivalent to the following statements\n\n\\[ Y_i = \\mu_i + e_i , \\ e_i \\sim N(0, \\sigma^2) \\]\n\nWe do not assume the collection of \\(Y_i, i=1, \\ldots, n\\) have a normal distribution\nInstead we assume the error term is normally distributed - a lesser assumption!\nIn case of multiple predictors, \\(\\mu_i\\) becomes a weighted average of the \\(\\beta_j\\) values, the regression coefficient with \\(x_{ij}\\) denoting the predictors. For example, for two covariates we have\n\n\\[ E(y_i) = \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} \\]\n\nA polynomial regression model of 2 degrees on \\(x_{i1}\\)\n\n\\[ \\mu_i = \\beta_0 + \\beta_1 x_{i1} +  \\beta_2 x_{i2} + \\beta_3 x_{i1}^2 \\]\n\nAssumptions of linear regression models\n\nIndependent observations\nLinear relationship We can check this assumption by examining marginal plots comparing the model predicted relationship between outcome and each continuous predictor and by examining the residual plot.\nNormality of the residuals\nHomoscedasticity Homoscedasticity in a model means that the residual is constant along the values of the dependent variable.\nMulticollinearity Multicollinearity is the phenomenon when a number of the explanatory variables are strongly correlated.\nCorrectly specified regression model This means that all relevant predictors for the response variable have been included in the model. This is often difficult to verify, but we can use posterior predictive distribution to check for regression fit.\n\n\n\n\n\nExample: Suppose we are interested to study the linear relationship between abundance and mass\n\n\n# Linear regression of maximum abundance against mass\nm1 <- lm(MaxAbund ~ Mass, data = bird)\nsummary(m1)\n\n\nCall:\nlm(formula = MaxAbund ~ Mass, data = bird)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-79.30 -35.39 -22.06   2.62 373.72 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 38.16646   11.09065   3.441  0.00115 **\nMass         0.01439    0.01059   1.358  0.18021   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 72.89 on 52 degrees of freedom\nMultiple R-squared:  0.03427,   Adjusted R-squared:  0.0157 \nF-statistic: 1.845 on 1 and 52 DF,  p-value: 0.1802\n\n# Verify regression assumptions using diagnostic plots\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\n\n\n\n\n\nPlot 1, Residuals vs Fitted shares information about the independence assumption\n\nresiduals should scattered randomly around the line of 0\n\nindicating linear relationship and the mean of the residuals is 0.\n\nresiduals form an approximate horizontal band around the 0 line\n\nindicating homogeneous\n\nIf the residuals are organized in a funnel shape, the residuals are not homoscedastic.\n\nPlot 2, QQ plot, assessing normality\nPlot 3, Scale location checks for residual variability and dispersion - homoscedasticity\n\na visble trend is problematic\n\nPlot 4, Residuals vs Leverage is used to identify influential observations, in other words, outliers.\n\n\n\n# testing normality of residuals using Shapiro-Wilk test;\n# The Shapiro-Wilk test compares the distribution of the observed data to a normal distribution. \nshapiro.test(residuals(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1)\nW = 0.64158, p-value = 0.0000000003172\n\n\n\nUpdate model\n\npotential strategies: log-transform data, fitting non-linear regression\n\n\n\n# log-transform the variables\nbird$logMaxAbund <- log10(bird$MaxAbund)\nbird$logMass <- log10(bird$Mass)\n\nm2 <- lm(logMaxAbund ~ logMass, data = bird)\nsummary(m2)\n\n\nCall:\nlm(formula = logMaxAbund ~ logMass, data = bird)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.93562 -0.39982  0.05487  0.40625  1.61469 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(>|t|)    \n(Intercept)   1.6724     0.2472   6.767 0.0000000117 ***\nlogMass      -0.2361     0.1170  -2.019       0.0487 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6959 on 52 degrees of freedom\nMultiple R-squared:  0.07267,   Adjusted R-squared:  0.05484 \nF-statistic: 4.075 on 1 and 52 DF,  p-value: 0.04869\n\npar(mfrow = c(2, 2))\nplot(m2)\n\n\n\n\n\n\n\n\n\nwe observed improvement!\n\n\nBeyond simple linear regression one can fit multivariate regression\nIn multivariate regression, model building also need to consider variable selection and model fit\n\nstepwise model selection, forward/backward, LASSO\nAIC, BIC"
  }
]