---
title: "3. Statistical analysis in R"
---

::: callout-tip
## Learning Objectives
-  learn how to conduct descriptive analysis in R
-  learn how to run regression analysis in r
:::


-  For this session, we will be working with the birdsdiet.csv dataset

-  This data contains abundance and mass of bird species, data is obtained online from <https://r.qcbs.ca/workshops/>

-  Dataset is used for demonstration purpose only


```{r message=FALSE, warning=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
options(scipen = 999, pillar.print_max = Inf)
bird <- read_csv(file = "birdsdiet.csv")
```

- display data

```{r warning=FALSE, message=FALSE}
library(DT)
datatable(bird)
str(bird)
```
## 3.1 Descriptive Analysis

-  Descriptive statistics
    -  measures of central tendency, variability, and distribution shape for continuous variables
    -  frequency counts for categorical variables
    
```{r}
# basic summary using summary function;
summary(bird)

# updating binary and categorical variables to factors in R;
bird <- bird %>%
  mutate(across(c(Family, Diet, Passerine, Aquatic), factor))

summary(bird)
```
```{r}
# writing our own summary functions;
mysummary <- function(x, na.omit=FALSE){                
  x <- x[!is.na(x)]                
  m <- mean(x)  
  median <- median(x)
  n <- length(x)                
  s <- sd(x)                
  skew <- sum((x-m)^3/s^3)/n                
  kurt <- sum((x-m)^4/s^4)/n - 3                
  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))}

sapply(bird[,c("MaxAbund", "AvgAbund", "Mass")],mysummary)
```

-  We can use the `describe.by()` function from the `psych` package to generate summary statistics by group

```{r}
library(psych)
describe.by(bird[,c("MaxAbund", "AvgAbund", "Mass")], bird$Diet)
```

-  Frequency counts for categorical variables

```{r}
table(bird$Diet)
table(bird$Diet, bird$Aquatic) #diet by whether or not the bird lives on or around water;
xtabs(~Diet + Aquatic, data = bird)
```
-  (Visual) Bar display of Abundance distribution 

```{r}
ggplot(bird) +
    geom_bar(aes(x=Diet, y=AvgAbund, fill = Aquatic), 
             stat="identity",
             position = position_dodge()) +
    theme_minimal()
  
```


-  Measures of independence between 

-  testing independence of the categorical variables
    -  chi-square test
    -  Fisher exact
    -  Cochran-Mantel–Haenszel (taking into account of confounding from a third variable)

```{r}
# testing independence between Diet and Aquatic;
mytable <- xtabs(~Diet + Aquatic, data = bird)
chisq.test(mytable)
fisher.test(mytable)

mytable <- xtabs(~Diet + Aquatic + Passerine, data=bird)
mantelhaen.test(mytable)
```

-  Measures of correlations
    - variety of correlation coefficients, including Pearson, Spearman, Kendall
        -  Pearson product moment correlation assesses the degree of **linear relationship** between two quantitative variables
        -  Spearman’s Rank Order correlation coefficient assesses the degree of relationship between two **rank-ordered variables** - **nonparametric**
        -  Kendall’s Tau is also a **nonparametric** measure of rank correlation
    - we can also view correlation descriptively using correlation plots
    
```{r warning=FALSE, message=FALSE}
cont_var <- bird[,c("MaxAbund", "AvgAbund", "Mass")]
cor(cont_var, method = "pearson")
cor(cont_var, method = "spearman")
cor(cont_var, method = "kendall")

# testing correlation significance;
cor.test(bird$MaxAbund, bird$Mass, method = "pearson", alternative = "two.side")
cor.test(bird$MaxAbund, bird$Mass, method = "spearman", alternative = "two.side")
cor.test(bird$MaxAbund, bird$Mass, method = "kendall", alternative = "two.side")

library(corrplot)
M <- cor(cont_var, method = "pearson")
corrplot(M, 
         method="circle",
         type = c("upper"),
         tl.srt=45) #Text label color and rotation; 
```


-  T-test, comparison of groups

-   A two-group independent t-test can be used to test the hypothesis that the two population means are equal

```{r}
 t.test(MaxAbund ~ Aquatic, data=bird)
```
-  Nonparametric tests of group difference
    -  If you’re unable to meet the parametric assumptions of a t-test or ANOVA, you can turn to nonparametric approaches, 
        -  Thomogeneity of variance (i.e., the variability of the data in each group is similar). 
        -  The distribution is approximately normal.
    -  e.g. Wilcoxon rank sum test (more popularly known as the Mann–Whitney U test) 
    -  whether the observations are sampled from the same probability distribution (that is, whether the probability of obtaining higher scores is greater in one population than the other)
    
```{r warning=FALSE, message=FALSE}
wilcox.test(MaxAbund ~ Aquatic, data=bird) 
```
-  Comparing more than two groups
    -  ANOVA
    -  Kruskal-Wallis test, **nonparametric**
    
```{r}
res.aov <- aov(MaxAbund ~ Diet, data=bird)
summary(res.aov)
kruskal.test(MaxAbund ~ Diet, data=bird)
```
## 3.2 Linear regression

**What is regression?**

- using one or more predictor variables to model the distribution of one or more outcome variables

- For a continuous outcome, we typically fit a linear regression model. 
    - Of course the relationship between outcome and predictors can be non-linear, in this case, we would consider fitting polynomial regression models or splines.
    
- For a categorical outcome, we will fit a generalized linear regression. We will cover this topic in future sessions.

- For a repeatedly measured outcome, we can fit a linear mixed-effect model (continuous outcome) or a generalized linear mixed-effect model (categorical outcome).


### Normal Models and Linear Regression

::: {.callout-tip}
## Conditional normal model

- Given its mean and variance, an observation has a normal distribution

$$ Y_i \mid \mu_i, \sigma^2_i \sim N( \mu_i, \sigma^2_i) $$

- This is equivalent to the following statements

$$ Y_i = \mu_i + e_i , \ e_i \sim N(0, \sigma^2) $$

- We do not assume the collection of $Y_i, i=1, \ldots, n$ have a normal distribution
- Instead we assume the error term is normally distributed - a lesser assumption!

- In case of multiple predictors, $\mu_i$ becomes a weighted average of the $\beta_j$ values, the regression coefficient with $x_{ij}$ denoting the predictors. For example, for two covariates we have

$$ E(y_i) = \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} $$

- A polynomial regression model of 2 degrees on $x_{i1}$

$$ \mu_i = \beta_0 + \beta_1 x_{i1} +  \beta_2 x_{i2} + \beta_3 x_{i1}^2 $$

- Assumptions of linear regression models
    1. **Independent observations** 
    2. **Linear relationship** We can check this assumption by examining marginal plots comparing the model predicted relationship between outcome and each continuous predictor and by examining the residual plot.
    3. **Normality of the residuals**
    4. **Homoscedasticity** Homoscedasticity in a model means that the residual is constant along the values of the dependent variable.
    5. **Multicollinearity** Multicollinearity is the phenomenon when a number of the explanatory variables are strongly correlated.
    6. **Correctly specified regression model** This means that all relevant predictors for the response variable have been included in the model. This is often difficult to verify, but we can use posterior predictive distribution to check for regression fit.
    
:::


-  Example: Suppose we are interested to study the linear relationship between abundance and mass

```{r}
# Linear regression of maximum abundance against mass
m1 <- lm(MaxAbund ~ Mass, data = bird)
summary(m1)

# Verify regression assumptions using diagnostic plots
par(mfrow = c(2, 2))
plot(m1)
```

-  Plot 1, Residuals vs Fitted shares information about the independence assumption
    -  residuals should scattered randomly around the line of 0
        -  indicating linear relationship and the mean of the residuals is 0.
    -  residuals form an approximate horizontal band around the 0 line
        -  indicating homogeneous
    -  If the residuals are organized in a funnel shape, the residuals are not homoscedastic.

-  Plot 2, QQ plot, assessing normality

-  Plot 3, Scale location checks for residual variability and dispersion - homoscedasticity
    -  a visble trend is problematic

-  Plot 4,  Residuals vs Leverage is used to identify influential observations, in other words, outliers.



<!-- Based on the insights gained in the previous sections, there are serious problems with these diagnostic plots. * Plots 1 and 2 shows strong trends, * Plot 3 shows that the residuals do not follow a normal distribution, * Plot 4 highlights the leverage of point 32 and its very high influence. -->

```{r}
# testing normality of residuals using Shapiro-Wilk test;
# The Shapiro-Wilk test compares the distribution of the observed data to a normal distribution. 
shapiro.test(residuals(m1))
```
-  Update model
    -  potential strategies: log-transform data, fitting non-linear regression
    
```{r}
# log-transform the variables
bird$logMaxAbund <- log10(bird$MaxAbund)
bird$logMass <- log10(bird$Mass)

m2 <- lm(logMaxAbund ~ logMass, data = bird)
summary(m2)
par(mfrow = c(2, 2))
plot(m2)
```

> we observed improvement!

-  Beyond simple linear regression one can fit multivariate regression
-  In multivariate regression, model building also need to consider variable selection and model fit
    -  stepwise model selection, forward/backward, LASSO 
    -  AIC, BIC
    
